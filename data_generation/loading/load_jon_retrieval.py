#!/usr/bin/env python
"""
Load Jon Data into Retrieval Store

This script loads the synthetic Jon data generated by jon_data_generator.py
into the retrieval store used by SchultzGPT with the OpenAI Retrieval API.

Usage:
    python -m data_generation.load_jon_retrieval --file path/to/jon_retrieval_data.jsonl
"""

import os
import sys
import json
import argparse
from tqdm import tqdm

# Add src to path for imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from dotenv import load_dotenv
from src.services.retrieval_store import RetrievalStore

# Load environment variables
load_dotenv()

def load_retrieval_data(file_path: str, retrieval_store: RetrievalStore, batch_size: int = 20) -> int:
    """Load data from a JSONL file into the retrieval store"""
    loaded_count = 0
    
    with open(file_path, 'r') as f:
        lines = f.readlines()
    
    # Prepare batches to optimize performance (OpenAI recommends batches)
    batch_texts = []
    batch_metadatas = []
    
    for line in tqdm(lines, desc="Processing entries"):
        try:
            entry = json.loads(line.strip())
            
            # Extract fields
            if 'text' in entry and 'metadata' in entry:
                batch_texts.append(entry['text'])
                batch_metadatas.append(entry['metadata'])
                
                # Process batch if it's full
                if len(batch_texts) >= batch_size:
                    if retrieval_store.add_batch(batch_texts, batch_metadatas):
                        loaded_count += len(batch_texts)
                    else:
                        print(f"Error adding batch starting at entry {loaded_count + 1}")
                    
                    # Reset batch
                    batch_texts = []
                    batch_metadatas = []
            else:
                print(f"Skipping entry: missing required fields - {entry}")
                
        except Exception as e:
            print(f"Error processing entry: {e}")
    
    # Process any remaining entries in the last batch
    if batch_texts:
        if retrieval_store.add_batch(batch_texts, batch_metadatas):
            loaded_count += len(batch_texts)
        else:
            print(f"Error adding final batch")
    
    # Save the retrieval store
    retrieval_store.save_store()
    
    return loaded_count

def main():
    parser = argparse.ArgumentParser(description="Load Jon data into retrieval store")
    parser.add_argument("--file", type=str, required=True,
                       help="Path to the JSONL file containing data to load")
    parser.add_argument("--assistant-id", type=str, default=None,
                       help="Optional OpenAI Assistant ID to use (creates new if not provided)")
    parser.add_argument("--dry-run", action="store_true",
                       help="Validate data without loading it")
    parser.add_argument("--batch-size", type=int, default=20,
                       help="Number of entries to process in each batch")
    args = parser.parse_args()
    
    # Check if file exists
    if not os.path.exists(args.file):
        print(f"Error: File not found: {args.file}")
        sys.exit(1)
    
    # Initialize retrieval store
    retrieval_store = RetrievalStore(
        assistant_id=args.assistant_id,
        embedding_model=os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
    )
    
    # Get assistant and file info
    assistant_id = retrieval_store.assistant_id
    file_count = len(retrieval_store.file_ids)
    print(f"Using Assistant ID: {assistant_id}")
    print(f"Current file count: {file_count}")
    
    if args.dry_run:
        print("Dry run mode - validating data without loading")
        # Count valid entries in the file
        valid_count = 0
        with open(args.file, 'r') as f:
            for line in tqdm(f, desc="Validating entries"):
                try:
                    entry = json.loads(line.strip())
                    if 'text' in entry and 'metadata' in entry:
                        valid_count += 1
                except:
                    pass
        print(f"File contains {valid_count} valid entries that would be loaded")
    else:
        # Load data
        loaded_count = load_retrieval_data(args.file, retrieval_store, args.batch_size)
        
        # Report results
        new_file_count = len(retrieval_store.file_ids)
        print(f"Successfully loaded {loaded_count} entries")
        print(f"Files added to Assistant: {new_file_count - file_count}")
        print(f"Total files in Assistant: {new_file_count}")
        print(f"Assistant ID: {retrieval_store.assistant_id}")
    
if __name__ == "__main__":
    main() 